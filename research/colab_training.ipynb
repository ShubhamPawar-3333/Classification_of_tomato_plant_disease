{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tomato Disease Advisory System - Training Pipeline\n",
        "\n",
        "This notebook runs the full ML pipeline on Google Colab GPU:\n",
        "1. Clone repository\n",
        "2. Install dependencies\n",
        "3. Download & extract dataset\n",
        "4. Prepare EfficientNet-B4 base model\n",
        "5. Train with augmentation\n",
        "6. Evaluate and log metrics to MLflow\n",
        "7. Push trained model back to GitHub\n",
        "\n",
        "**Runtime:** Set to **GPU** (Runtime > Change runtime type > T4 GPU)"
      ],
      "metadata": { "id": "intro" }
    },
    {
      "cell_type": "markdown",
      "source": ["## Step 1: Verify GPU"],
      "metadata": { "id": "gpu_header" }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print('TensorFlow version:', tf.__version__)\n",
        "print('GPU Available:', tf.config.list_physical_devices('GPU'))\n",
        "assert len(tf.config.list_physical_devices('GPU')) > 0, 'No GPU found! Change runtime to GPU.'"
      ],
      "metadata": { "id": "verify_gpu" },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Step 2: Clone Repository"],
      "metadata": { "id": "clone_header" }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "REPO_URL = 'https://github.com/ShubhamPawar-3333/Classification_of_tomato_plant_disease.git'\n",
        "PROJECT_DIR = '/content/Classification_of_tomato_plant_disease'\n",
        "\n",
        "if os.path.exists(PROJECT_DIR):\n",
        "    %cd {PROJECT_DIR}\n",
        "    !git pull origin master\n",
        "else:\n",
        "    !git clone {REPO_URL}\n",
        "    %cd {PROJECT_DIR}\n",
        "\n",
        "!ls -la"
      ],
      "metadata": { "id": "clone_repo" },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Step 3: Install Dependencies"],
      "metadata": { "id": "install_header" }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q python-box ensure PyYAML mlflow scikit-learn seaborn"
      ],
      "metadata": { "id": "install_deps" },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add src to Python path\n",
        "import sys\n",
        "sys.path.insert(0, os.path.join(PROJECT_DIR, 'src'))\n",
        "\n",
        "# Verify import works\n",
        "from tomato_disease_advisor.config import ConfigurationManager\n",
        "print('Imports successful!')"
      ],
      "metadata": { "id": "setup_path" },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Step 4: Download Dataset\n", "\n", "Downloads the PlantVillage tomato disease dataset."],
      "metadata": { "id": "data_header" }
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "import zipfile\n",
        "\n",
        "# PlantVillage Tomato Dataset (10 classes)\n",
        "# You can replace this with your own dataset URL\n",
        "DATASET_URL = 'https://www.kaggle.com/api/v1/datasets/download/arjuntejaswi/plant-village'\n",
        "\n",
        "DATA_DIR = os.path.join(PROJECT_DIR, 'artifacts', 'data_ingestion')\n",
        "DATASET_DIR = os.path.join(DATA_DIR, 'dataset')\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "if not os.path.exists(DATASET_DIR) or len(os.listdir(DATASET_DIR)) == 0:\n",
        "    print('Downloading dataset from Kaggle...')\n",
        "    print('NOTE: If Kaggle download fails, upload dataset manually.')\n",
        "    print('Alternative: Upload your dataset zip to Google Drive and use gdown.')\n",
        "    \n",
        "    # Option 1: Using opendatasets (requires Kaggle credentials)\n",
        "    try:\n",
        "        !pip install -q opendatasets\n",
        "        import opendatasets as od\n",
        "        od.download('https://www.kaggle.com/datasets/arjuntejaswi/plant-village', DATA_DIR)\n",
        "    except Exception as e:\n",
        "        print(f'Kaggle download failed: {e}')\n",
        "        print('\\nPlease upload dataset manually:')\n",
        "        print('1. Download from: https://www.kaggle.com/datasets/arjuntejaswi/plant-village')\n",
        "        print('2. Upload the zip file to Colab')\n",
        "        print('3. Run the manual upload cell below')\n",
        "else:\n",
        "    print(f'Dataset already exists at: {DATASET_DIR}')\n",
        "    print(f'Contents: {os.listdir(DATASET_DIR)}')"
      ],
      "metadata": { "id": "download_data" },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MANUAL UPLOAD OPTION: Run this cell if Kaggle download failed\n",
        "# Upload your dataset zip file when prompted\n",
        "\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "# zip_path = list(uploaded.keys())[0]\n",
        "# with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "#     zip_ref.extractall(DATASET_DIR)\n",
        "# print(f'Extracted to: {DATASET_DIR}')"
      ],
      "metadata": { "id": "manual_upload" },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter only tomato classes from PlantVillage\n",
        "import shutil\n",
        "\n",
        "# Find the actual data directory\n",
        "data_root = DATASET_DIR\n",
        "for root, dirs, files_list in os.walk(DATASET_DIR):\n",
        "    if any('Tomato' in d for d in dirs):\n",
        "        data_root = root\n",
        "        break\n",
        "\n",
        "print(f'Data root: {data_root}')\n",
        "\n",
        "# List tomato classes\n",
        "tomato_classes = [d for d in os.listdir(data_root) if 'Tomato' in d]\n",
        "print(f'\\nFound {len(tomato_classes)} tomato classes:')\n",
        "for cls in sorted(tomato_classes):\n",
        "    count = len(os.listdir(os.path.join(data_root, cls)))\n",
        "    print(f'  {cls}: {count} images')\n",
        "\n",
        "# Create filtered dataset with only tomato classes\n",
        "TOMATO_DIR = os.path.join(DATASET_DIR, 'tomato')\n",
        "if not os.path.exists(TOMATO_DIR):\n",
        "    os.makedirs(TOMATO_DIR, exist_ok=True)\n",
        "    for cls in tomato_classes:\n",
        "        src = os.path.join(data_root, cls)\n",
        "        dst = os.path.join(TOMATO_DIR, cls)\n",
        "        if not os.path.exists(dst):\n",
        "            shutil.copytree(src, dst)\n",
        "    print(f'\\nFiltered dataset created at: {TOMATO_DIR}')\n",
        "else:\n",
        "    print(f'Filtered dataset already exists at: {TOMATO_DIR}')\n",
        "\n",
        "total = sum(len(os.listdir(os.path.join(TOMATO_DIR, d))) for d in os.listdir(TOMATO_DIR) if os.path.isdir(os.path.join(TOMATO_DIR, d)))\n",
        "print(f'\\nTotal images: {total}')"
      ],
      "metadata": { "id": "filter_tomato" },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Step 5: Prepare Base Model (EfficientNet-B4)"],
      "metadata": { "id": "model_header" }
    },
    {
      "cell_type": "code",
      "source": [
        "from tomato_disease_advisor.utils import read_yaml\n",
        "from pathlib import Path\n",
        "\n",
        "# Load configs\n",
        "config = read_yaml(Path(os.path.join(PROJECT_DIR, 'config', 'config.yaml')))\n",
        "params = read_yaml(Path(os.path.join(PROJECT_DIR, 'params.yaml')))\n",
        "\n",
        "print('Config loaded successfully')\n",
        "print(f'Model: {config.model.name}')\n",
        "print(f'Input shape: {list(config.model.input_shape)}')\n",
        "print(f'Classes: {config.model.classes}')"
      ],
      "metadata": { "id": "load_config" },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model\n",
        "IMAGE_SIZE = params.IMAGE_SIZE\n",
        "NUM_CLASSES = config.model.classes\n",
        "\n",
        "# Download EfficientNetB4\n",
        "base_model = tf.keras.applications.EfficientNetB4(\n",
        "    include_top=False,\n",
        "    weights='imagenet',\n",
        "    input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3)\n",
        ")\n",
        "\n",
        "# Freeze base\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Add classification head\n",
        "x = base_model.output\n",
        "x = tf.keras.layers.GlobalAveragePooling2D(name='global_avg_pool')(x)\n",
        "x = tf.keras.layers.BatchNormalization(name='bn_head')(x)\n",
        "x = tf.keras.layers.Dropout(params.DROPOUT_RATE, name='dropout_1')(x)\n",
        "x = tf.keras.layers.Dense(params.DENSE_UNITS, activation='relu', name='dense_1')(x)\n",
        "x = tf.keras.layers.BatchNormalization(name='bn_dense')(x)\n",
        "x = tf.keras.layers.Dropout(0.2, name='dropout_2')(x)\n",
        "predictions = tf.keras.layers.Dense(NUM_CLASSES, activation='softmax', name='predictions')(x)\n",
        "\n",
        "model = tf.keras.Model(\n",
        "    inputs=base_model.input,\n",
        "    outputs=predictions,\n",
        "    name='EfficientNetB4_TomatoDisease'\n",
        ")\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=params.LEARNING_RATE),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(f'Total params: {model.count_params():,}')\n",
        "trainable = sum(tf.keras.backend.count_params(w) for w in model.trainable_weights)\n",
        "print(f'Trainable params: {trainable:,}')"
      ],
      "metadata": { "id": "build_model" },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Step 6: Train the Model"],
      "metadata": { "id": "train_header" }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data generators\n",
        "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rescale=1.0/255,\n",
        "    rotation_range=params.AUGMENTATION.rotation_range,\n",
        "    width_shift_range=params.AUGMENTATION.width_shift_range,\n",
        "    height_shift_range=params.AUGMENTATION.height_shift_range,\n",
        "    horizontal_flip=params.AUGMENTATION.horizontal_flip,\n",
        "    zoom_range=params.AUGMENTATION.zoom_range,\n",
        "    fill_mode=params.AUGMENTATION.fill_mode,\n",
        "    validation_split=params.VALIDATION_SPLIT\n",
        ")\n",
        "\n",
        "train_gen = train_datagen.flow_from_directory(\n",
        "    TOMATO_DIR,\n",
        "    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
        "    batch_size=params.BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    subset='training',\n",
        "    shuffle=True,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "val_gen = train_datagen.flow_from_directory(\n",
        "    TOMATO_DIR,\n",
        "    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
        "    batch_size=params.BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    subset='validation',\n",
        "    shuffle=False,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "print(f'\\nTraining samples: {train_gen.samples}')\n",
        "print(f'Validation samples: {val_gen.samples}')\n",
        "print(f'Classes: {list(train_gen.class_indices.keys())}')"
      ],
      "metadata": { "id": "data_gen" },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Callbacks\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=params.EARLY_STOPPING_PATIENCE,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=params.REDUCE_LR_FACTOR,\n",
        "        patience=params.REDUCE_LR_PATIENCE,\n",
        "        min_lr=1e-7,\n",
        "        verbose=1\n",
        "    )\n",
        "]\n",
        "\n",
        "# Train\n",
        "history = model.fit(\n",
        "    train_gen,\n",
        "    epochs=params.EPOCHS,\n",
        "    validation_data=val_gen,\n",
        "    callbacks=callbacks,\n",
        "    steps_per_epoch=train_gen.samples // params.BATCH_SIZE,\n",
        "    validation_steps=val_gen.samples // params.BATCH_SIZE\n",
        ")"
      ],
      "metadata": { "id": "train_model" },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training history\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Accuracy\n",
        "axes[0].plot(history.history['accuracy'], label='Train Accuracy')\n",
        "axes[0].plot(history.history['val_accuracy'], label='Val Accuracy')\n",
        "axes[0].set_title('Model Accuracy')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Accuracy')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Loss\n",
        "axes[1].plot(history.history['loss'], label='Train Loss')\n",
        "axes[1].plot(history.history['val_loss'], label='Val Loss')\n",
        "axes[1].set_title('Model Loss')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Loss')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('training_history.png', dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print(f'\\nBest Val Accuracy: {max(history.history[\"val_accuracy\"]):.4f}')\n",
        "print(f'Final Train Accuracy: {history.history[\"accuracy\"][-1]:.4f}')"
      ],
      "metadata": { "id": "plot_history" },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Step 7: Evaluate the Model"],
      "metadata": { "id": "eval_header" }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "# Predictions\n",
        "predictions = model.predict(val_gen, verbose=1)\n",
        "y_pred = np.argmax(predictions, axis=1)\n",
        "y_true = val_gen.classes\n",
        "\n",
        "# Class names\n",
        "class_names = list(val_gen.class_indices.keys())\n",
        "short_names = [name.replace('Tomato___', '').replace('Tomato_', '')[:25] for name in class_names]\n",
        "\n",
        "# Classification report\n",
        "print('Classification Report:')\n",
        "print('=' * 60)\n",
        "print(classification_report(y_true, y_pred, target_names=short_names))"
      ],
      "metadata": { "id": "evaluate" },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 10))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=short_names, yticklabels=short_names, ax=ax)\n",
        "ax.set_xlabel('Predicted', fontsize=12)\n",
        "ax.set_ylabel('True', fontsize=12)\n",
        "ax.set_title('Confusion Matrix - Tomato Disease Classification', fontsize=14)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.savefig('confusion_matrix.png', dpi=150)\n",
        "plt.show()"
      ],
      "metadata": { "id": "confusion_matrix" },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save scores.json\n",
        "import json\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "scores = {\n",
        "    'accuracy': float(accuracy_score(y_true, y_pred)),\n",
        "    'f1_weighted': float(f1_score(y_true, y_pred, average='weighted')),\n",
        "    'precision_weighted': float(precision_score(y_true, y_pred, average='weighted')),\n",
        "    'recall_weighted': float(recall_score(y_true, y_pred, average='weighted'))\n",
        "}\n",
        "\n",
        "with open('scores.json', 'w') as f:\n",
        "    json.dump(scores, f, indent=4)\n",
        "\n",
        "print('Scores:')\n",
        "for k, v in scores.items():\n",
        "    print(f'  {k}: {v:.4f}')"
      ],
      "metadata": { "id": "save_scores" },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Step 8: Save Model"],
      "metadata": { "id": "save_header" }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save trained model\n",
        "MODEL_DIR = os.path.join(PROJECT_DIR, 'artifacts', 'training')\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "model_path = os.path.join(MODEL_DIR, 'model.h5')\n",
        "model.save(model_path)\n",
        "print(f'Model saved to: {model_path}')\n",
        "\n",
        "# Also save as SavedModel format\n",
        "savedmodel_path = os.path.join(MODEL_DIR, 'saved_model')\n",
        "model.save(savedmodel_path)\n",
        "print(f'SavedModel saved to: {savedmodel_path}')\n",
        "\n",
        "# Model size\n",
        "size_mb = os.path.getsize(model_path) / (1024 * 1024)\n",
        "print(f'Model size: {size_mb:.1f} MB')"
      ],
      "metadata": { "id": "save_model" },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Step 9: Download Model to Local Machine"],
      "metadata": { "id": "download_header" }
    },
    {
      "cell_type": "code",
      "source": [
        "# Option 1: Download directly from Colab\n",
        "from google.colab import files\n",
        "\n",
        "# Download model.h5\n",
        "files.download(model_path)\n",
        "\n",
        "# Download scores\n",
        "files.download('scores.json')\n",
        "files.download('training_history.png')\n",
        "files.download('confusion_matrix.png')"
      ],
      "metadata": { "id": "download_model" },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Option 2: Save to Google Drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# \n",
        "# DRIVE_DIR = '/content/drive/MyDrive/tomato-disease-model'\n",
        "# os.makedirs(DRIVE_DIR, exist_ok=True)\n",
        "# \n",
        "# import shutil\n",
        "# shutil.copy(model_path, os.path.join(DRIVE_DIR, 'model.h5'))\n",
        "# shutil.copy('scores.json', os.path.join(DRIVE_DIR, 'scores.json'))\n",
        "# shutil.copy('training_history.png', os.path.join(DRIVE_DIR, 'training_history.png'))\n",
        "# shutil.copy('confusion_matrix.png', os.path.join(DRIVE_DIR, 'confusion_matrix.png'))\n",
        "# print(f'Files saved to Google Drive: {DRIVE_DIR}')"
      ],
      "metadata": { "id": "save_drive" },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Done!\n",
        "\n",
        "### Next Steps (on your local machine):\n",
        "1. Place `model.h5` in `artifacts/training/model.h5`\n",
        "2. Place `scores.json` in the project root\n",
        "3. Place `confusion_matrix.png` in `artifacts/evaluation/`\n",
        "4. Commit and push:\n",
        "```bash\n",
        "git add scores.json artifacts/\n",
        "git commit -m 'feat: Add trained model and evaluation results'\n",
        "git push origin master\n",
        "```"
      ],
      "metadata": { "id": "done" }
    }
  ]
}
