{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tomato Disease Advisory System \u2014 Training Pipeline (v3)\n",
        "\n",
        "**Fixes from v2:**\n",
        "- \u2705 Fixed preprocessing: `preprocess_input` instead of `rescale=1/255`\n",
        "- \u2705 Separate validation generator (no augmentation)\n",
        "- \u2705 Dynamic EfficientNet selection based on IMAGE_SIZE\n",
        "- \u2705 Stabilized LR: Phase 1 = 3e-4, Phase 2 = 1e-4\n",
        "- \u2705 Simplified classification head\n",
        "- \u2705 Pre-training sanity checks\n",
        "\n",
        "**Runtime:** Set to **GPU** (Runtime > Change runtime type > T4 GPU)"
      ],
      "metadata": {
        "id": "intro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Verify GPU"
      ],
      "metadata": {
        "id": "gpu_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print('TensorFlow version:', tf.__version__)\n",
        "print('GPU Available:', tf.config.list_physical_devices('GPU'))\n",
        "assert len(tf.config.list_physical_devices('GPU')) > 0, 'No GPU found! Change runtime to GPU.'"
      ],
      "metadata": {
        "id": "verify_gpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Clone Repository"
      ],
      "metadata": {
        "id": "clone_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "REPO_URL = 'https://github.com/ShubhamPawar-3333/Classification_of_tomato_plant_disease.git'\n",
        "PROJECT_DIR = '/content/Classification_of_tomato_plant_disease'\n",
        "\n",
        "if os.path.exists(PROJECT_DIR):\n",
        "    %cd {PROJECT_DIR}\n",
        "    !git pull origin master\n",
        "else:\n",
        "    !git clone {REPO_URL}\n",
        "    %cd {PROJECT_DIR}\n",
        "\n",
        "!ls -la"
      ],
      "metadata": {
        "id": "clone_repo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Install Dependencies"
      ],
      "metadata": {
        "id": "install_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q python-box ensure PyYAML mlflow scikit-learn seaborn"
      ],
      "metadata": {
        "id": "install_deps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add src to Python path\n",
        "import sys\n",
        "sys.path.insert(0, os.path.join(PROJECT_DIR, 'src'))\n",
        "\n",
        "from tomato_disease_advisor.utils import read_yaml\n",
        "from pathlib import Path\n",
        "\n",
        "# Load configs\n",
        "config = read_yaml(Path(os.path.join(PROJECT_DIR, 'config', 'config.yaml')))\n",
        "params = read_yaml(Path(os.path.join(PROJECT_DIR, 'params.yaml')))\n",
        "print('Config loaded successfully')"
      ],
      "metadata": {
        "id": "setup_path"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Download Dataset"
      ],
      "metadata": {
        "id": "data_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "DATA_DIR = os.path.join(PROJECT_DIR, 'artifacts', 'data_ingestion')\n",
        "DATASET_DIR = os.path.join(DATA_DIR, 'dataset')\n",
        "os.makedirs(DATASET_DIR, exist_ok=True)\n",
        "\n",
        "if not os.path.exists(DATASET_DIR) or len(os.listdir(DATASET_DIR)) == 0:\n",
        "    print('Downloading dataset from Kaggle...')\n",
        "    try:\n",
        "        !pip install -q opendatasets\n",
        "        import opendatasets as od\n",
        "        od.download('https://www.kaggle.com/datasets/arjuntejaswi/plant-village', DATASET_DIR)\n",
        "    except Exception as e:\n",
        "        print(f'Kaggle download failed: {e}')\n",
        "        print('\\nPlease upload dataset manually using the cell below.')\n",
        "else:\n",
        "    print(f'Dataset already exists at: {DATASET_DIR}')"
      ],
      "metadata": {
        "id": "download_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MANUAL UPLOAD OPTION: Uncomment and run if Kaggle download failed\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "# zip_path = list(uploaded.keys())[0]\n",
        "# with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "#     zip_ref.extractall(DATASET_DIR)\n",
        "# print(f'Extracted to: {DATASET_DIR}')"
      ],
      "metadata": {
        "id": "manual_upload"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter only tomato classes from PlantVillage\n",
        "import shutil\n",
        "\n",
        "# Find the actual data directory\n",
        "data_root = DATASET_DIR\n",
        "for root, dirs, files_list in os.walk(DATASET_DIR):\n",
        "    if any('Tomato' in d for d in dirs):\n",
        "        data_root = root\n",
        "        break\n",
        "\n",
        "print(f'Data root: {data_root}')\n",
        "\n",
        "# List tomato classes\n",
        "tomato_classes = [d for d in os.listdir(data_root) if 'Tomato' in d]\n",
        "print(f'\\nFound {len(tomato_classes)} tomato classes:')\n",
        "total_images = 0\n",
        "for cls in sorted(tomato_classes):\n",
        "    count = len(os.listdir(os.path.join(data_root, cls)))\n",
        "    total_images += count\n",
        "    print(f'  {cls}: {count} images')\n",
        "\n",
        "# Create filtered dataset with only tomato classes\n",
        "TOMATO_DIR = os.path.join(DATASET_DIR, 'tomato')\n",
        "if not os.path.exists(TOMATO_DIR):\n",
        "    os.makedirs(TOMATO_DIR, exist_ok=True)\n",
        "    for cls in tomato_classes:\n",
        "        src = os.path.join(data_root, cls)\n",
        "        dst = os.path.join(TOMATO_DIR, cls)\n",
        "        if not os.path.exists(dst):\n",
        "            shutil.copytree(src, dst)\n",
        "    print(f'\\nFiltered dataset created at: {TOMATO_DIR}')\n",
        "else:\n",
        "    print(f'Filtered dataset already exists at: {TOMATO_DIR}')\n",
        "\n",
        "print(f'\\nTotal images: {total_images}')"
      ],
      "metadata": {
        "id": "filter_tomato"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Dynamic EfficientNet Selection\n",
        "\n",
        "Backbone is chosen **automatically** based on `IMAGE_SIZE` in `params.yaml`:\n",
        "- 224 \u2192 EfficientNetB0\n",
        "- 240 \u2192 EfficientNetB1\n",
        "- 260 \u2192 EfficientNetB2\n",
        "- 300 \u2192 EfficientNetB3\n",
        "- 380 \u2192 EfficientNetB4"
      ],
      "metadata": {
        "id": "model_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SIZE = params.IMAGE_SIZE\n",
        "NUM_CLASSES = config.model.classes\n",
        "\n",
        "# Dynamic backbone selection based on IMAGE_SIZE\n",
        "EFFICIENTNET_MAP = {\n",
        "    224: ('EfficientNetB0', tf.keras.applications.EfficientNetB0),\n",
        "    240: ('EfficientNetB1', tf.keras.applications.EfficientNetB1),\n",
        "    260: ('EfficientNetB2', tf.keras.applications.EfficientNetB2),\n",
        "    300: ('EfficientNetB3', tf.keras.applications.EfficientNetB3),\n",
        "    380: ('EfficientNetB4', tf.keras.applications.EfficientNetB4),\n",
        "}\n",
        "\n",
        "assert IMAGE_SIZE in EFFICIENTNET_MAP, (\n",
        "    f'IMAGE_SIZE={IMAGE_SIZE} not supported. Valid: {list(EFFICIENTNET_MAP.keys())}'\n",
        ")\n",
        "\n",
        "backbone_name, backbone_fn = EFFICIENTNET_MAP[IMAGE_SIZE]\n",
        "print(f'[Backbone] Selected: {backbone_name} (IMAGE_SIZE={IMAGE_SIZE})')\n",
        "print(f'[Backbone] Preprocessing: tf.keras.applications.efficientnet.preprocess_input')\n",
        "\n",
        "# Download base model\n",
        "base_model = backbone_fn(\n",
        "    include_top=False,\n",
        "    weights='imagenet',\n",
        "    input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3)\n",
        ")\n",
        "\n",
        "# Freeze ALL base layers initially (Phase 1)\n",
        "base_model.trainable = False\n",
        "\n",
        "# Simplified classification head: GAP -> Dropout -> Dense -> Output\n",
        "x = base_model.output\n",
        "x = tf.keras.layers.GlobalAveragePooling2D(name='global_avg_pool')(x)\n",
        "x = tf.keras.layers.Dropout(params.DROPOUT_RATE, name='dropout_1')(x)\n",
        "x = tf.keras.layers.Dense(params.DENSE_UNITS, activation='relu', name='dense_1')(x)\n",
        "predictions = tf.keras.layers.Dense(NUM_CLASSES, activation='softmax', name='predictions')(x)\n",
        "\n",
        "model = tf.keras.Model(\n",
        "    inputs=base_model.input,\n",
        "    outputs=predictions,\n",
        "    name=f'{backbone_name}_TomatoDisease'\n",
        ")\n",
        "\n",
        "print(f'\\nBase model layers: {len(base_model.layers)}')\n",
        "print(f'Total params: {model.count_params():,}')\n",
        "trainable = sum(tf.keras.backend.count_params(w) for w in model.trainable_weights)\n",
        "print(f'Trainable params: {trainable:,}')"
      ],
      "metadata": {
        "id": "build_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Prepare Data Generators\n",
        "\n",
        "**Key fixes:**\n",
        "- Uses `preprocess_input` (NOT `rescale=1/255`)\n",
        "- **Separate** validation generator with NO augmentation"
      ],
      "metadata": {
        "id": "datagen_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
        "\n",
        "# Training generator: augmentation + EfficientNet preprocessing\n",
        "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input,\n",
        "    rotation_range=params.AUGMENTATION.rotation_range,\n",
        "    width_shift_range=params.AUGMENTATION.width_shift_range,\n",
        "    height_shift_range=params.AUGMENTATION.height_shift_range,\n",
        "    horizontal_flip=params.AUGMENTATION.horizontal_flip,\n",
        "    zoom_range=params.AUGMENTATION.zoom_range,\n",
        "    fill_mode=params.AUGMENTATION.fill_mode,\n",
        "    validation_split=params.VALIDATION_SPLIT\n",
        ")\n",
        "\n",
        "# Validation generator: EfficientNet preprocessing ONLY (no augmentation!)\n",
        "val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input,\n",
        "    validation_split=params.VALIDATION_SPLIT\n",
        ")\n",
        "\n",
        "train_gen = train_datagen.flow_from_directory(\n",
        "    TOMATO_DIR,\n",
        "    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
        "    batch_size=params.BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    subset='training',\n",
        "    shuffle=True,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "val_gen = val_datagen.flow_from_directory(\n",
        "    TOMATO_DIR,\n",
        "    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
        "    batch_size=params.BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    subset='validation',\n",
        "    shuffle=False,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "print(f'\\nTraining samples: {train_gen.samples}')\n",
        "print(f'Validation samples: {val_gen.samples}')\n",
        "print(f'Classes: {list(train_gen.class_indices.keys())}')\n",
        "print(f'Preprocessing: preprocess_input (NOT rescale=1/255)')\n",
        "print(f'Validation augmentation: NONE (correct)')\n",
        "\n",
        "# Sanity checks\n",
        "assert IMAGE_SIZE in [224, 240, 260, 300, 380]\n",
        "assert NUM_CLASSES == train_gen.num_classes, (\n",
        "    f'Class mismatch! Config has {NUM_CLASSES} but data has {train_gen.num_classes}'\n",
        ")\n",
        "print(f'\\n\\u2713 Sanity checks passed')"
      ],
      "metadata": {
        "id": "data_gen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Phase 1 \u2014 Train Head Only\n",
        "\n",
        "Base model frozen. LR = 3e-4 (stabilized for EfficientNet).\n",
        "10 epochs gives the head a strong starting point."
      ],
      "metadata": {
        "id": "phase1_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PHASE1_EPOCHS = 10\n",
        "PHASE1_LR = 3e-4  # Stabilized (not 1e-3 which is too aggressive for EfficientNet)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=PHASE1_LR),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(f'Phase 1: Training head only ({PHASE1_EPOCHS} epochs, LR={PHASE1_LR})')\n",
        "print(f'Trainable params: {sum(tf.keras.backend.count_params(w) for w in model.trainable_weights):,}')\n",
        "print()\n",
        "\n",
        "history_phase1 = model.fit(\n",
        "    train_gen,\n",
        "    epochs=PHASE1_EPOCHS,\n",
        "    validation_data=val_gen,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(\n",
        "            monitor='val_loss', patience=5,\n",
        "            restore_best_weights=True, verbose=1\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(f'\\nPhase 1 Results:')\n",
        "print(f'  Train Accuracy: {history_phase1.history[\"accuracy\"][-1]:.4f}')\n",
        "print(f'  Val Accuracy:   {history_phase1.history[\"val_accuracy\"][-1]:.4f}')"
      ],
      "metadata": {
        "id": "phase1_train"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Phase 2 \u2014 Fine-Tune Top Layers\n",
        "\n",
        "Unfreeze top 25% of base model. LR = 1e-4 to preserve pretrained weights."
      ],
      "metadata": {
        "id": "phase2_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PHASE2_EPOCHS = 20\n",
        "PHASE2_LR = 1e-4\n",
        "UNFREEZE_FROM = int(len(base_model.layers) * 0.75)\n",
        "\n",
        "base_model.trainable = True\n",
        "for layer in base_model.layers[:UNFREEZE_FROM]:\n",
        "    layer.trainable = False\n",
        "\n",
        "unfrozen = sum(1 for layer in base_model.layers if layer.trainable)\n",
        "print(f'Unfroze {unfrozen} / {len(base_model.layers)} base model layers')\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=PHASE2_LR),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "trainable_count = sum(tf.keras.backend.count_params(w) for w in model.trainable_weights)\n",
        "print(f'Trainable params now: {trainable_count:,}')\n",
        "print(f'\\nPhase 2: Fine-tuning ({PHASE2_EPOCHS} epochs, LR={PHASE2_LR})')\n",
        "\n",
        "history_phase2 = model.fit(\n",
        "    train_gen,\n",
        "    epochs=PHASE2_EPOCHS,\n",
        "    validation_data=val_gen,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=params.EARLY_STOPPING_PATIENCE,\n",
        "            restore_best_weights=True,\n",
        "            verbose=1\n",
        "        ),\n",
        "        tf.keras.callbacks.ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=params.REDUCE_LR_FACTOR,\n",
        "            patience=params.REDUCE_LR_PATIENCE,\n",
        "            min_lr=1e-7,\n",
        "            verbose=1\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(f'\\nPhase 2 Results:')\n",
        "print(f'  Train Accuracy: {history_phase2.history[\"accuracy\"][-1]:.4f}')\n",
        "print(f'  Val Accuracy:   {history_phase2.history[\"val_accuracy\"][-1]:.4f}')\n",
        "print(f'  Best Val Acc:   {max(history_phase2.history[\"val_accuracy\"]):.4f}')"
      ],
      "metadata": {
        "id": "phase2_train"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 9: Training History"
      ],
      "metadata": {
        "id": "history_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Combine histories\n",
        "acc = history_phase1.history['accuracy'] + history_phase2.history['accuracy']\n",
        "val_acc = history_phase1.history['val_accuracy'] + history_phase2.history['val_accuracy']\n",
        "loss = history_phase1.history['loss'] + history_phase2.history['loss']\n",
        "val_loss = history_phase1.history['val_loss'] + history_phase2.history['val_loss']\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "axes[0].plot(acc, label='Train Accuracy', linewidth=2)\n",
        "axes[0].plot(val_acc, label='Val Accuracy', linewidth=2)\n",
        "axes[0].axvline(x=len(history_phase1.history['accuracy'])-0.5, color='r',\n",
        "               linestyle='--', alpha=0.7, label='Phase 1 \\u2192 2')\n",
        "axes[0].set_title('Model Accuracy', fontsize=14)\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Accuracy')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1].plot(loss, label='Train Loss', linewidth=2)\n",
        "axes[1].plot(val_loss, label='Val Loss', linewidth=2)\n",
        "axes[1].axvline(x=len(history_phase1.history['loss'])-0.5, color='r',\n",
        "               linestyle='--', alpha=0.7, label='Phase 1 \\u2192 2')\n",
        "axes[1].set_title('Model Loss', fontsize=14)\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Loss')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('training_history.png', dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print(f'\\nFinal Best Val Accuracy: {max(val_acc):.4f}')"
      ],
      "metadata": {
        "id": "plot_history"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 10: Evaluate"
      ],
      "metadata": {
        "id": "eval_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
        "import seaborn as sns\n",
        "\n",
        "# Sanity checks before evaluation\n",
        "print(f'Generator class indices: {val_gen.class_indices}')\n",
        "print(f'Model output shape: {model.output_shape}')\n",
        "print(f'Model output classes: {model.output_shape[-1]}')\n",
        "assert val_gen.num_classes == model.output_shape[-1], 'Class count mismatch!'\n",
        "print('\\u2713 Evaluation sanity checks passed\\n')\n",
        "\n",
        "# Reset generator before prediction for deterministic results\n",
        "val_gen.reset()\n",
        "\n",
        "# Predictions\n",
        "predictions = model.predict(val_gen, verbose=1)\n",
        "y_pred = np.argmax(predictions, axis=1)\n",
        "y_true = val_gen.classes\n",
        "\n",
        "# Class names from generator (ground truth)\n",
        "class_names = list(val_gen.class_indices.keys())\n",
        "short_names = [name.replace('Tomato___', '').replace('Tomato_', '')[:25] for name in class_names]\n",
        "\n",
        "print('Classification Report:')\n",
        "print('=' * 60)\n",
        "print(classification_report(y_true, y_pred, target_names=short_names))"
      ],
      "metadata": {
        "id": "evaluate"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 10))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=short_names, yticklabels=short_names, ax=ax)\n",
        "ax.set_xlabel('Predicted', fontsize=12)\n",
        "ax.set_ylabel('True', fontsize=12)\n",
        "ax.set_title('Confusion Matrix - Tomato Disease Classification', fontsize=14)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.savefig('confusion_matrix.png', dpi=150)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "confusion_matrix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save scores.json\n",
        "import json\n",
        "\n",
        "scores = {\n",
        "    'accuracy': float(accuracy_score(y_true, y_pred)),\n",
        "    'f1_weighted': float(f1_score(y_true, y_pred, average='weighted')),\n",
        "    'precision_weighted': float(precision_score(y_true, y_pred, average='weighted')),\n",
        "    'recall_weighted': float(recall_score(y_true, y_pred, average='weighted'))\n",
        "}\n",
        "\n",
        "with open('scores.json', 'w') as f:\n",
        "    json.dump(scores, f, indent=4)\n",
        "\n",
        "print('Scores:')\n",
        "for k, v in scores.items():\n",
        "    print(f'  {k}: {v:.4f}')"
      ],
      "metadata": {
        "id": "save_scores"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 11: Save & Download Model"
      ],
      "metadata": {
        "id": "save_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_DIR = os.path.join(PROJECT_DIR, 'artifacts', 'training')\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "model_path = os.path.join(MODEL_DIR, 'model.keras')\n",
        "model.save(model_path)\n",
        "print(f'Model saved to: {model_path}')\n",
        "\n",
        "size_mb = os.path.getsize(model_path) / (1024 * 1024)\n",
        "print(f'Model size: {size_mb:.1f} MB')"
      ],
      "metadata": {
        "id": "save_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download(model_path)\n",
        "files.download('scores.json')\n",
        "files.download('training_history.png')\n",
        "files.download('confusion_matrix.png')"
      ],
      "metadata": {
        "id": "download_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Option 2: Save to Google Drive (uncomment to use)\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# DRIVE_DIR = '/content/drive/MyDrive/tomato-disease-model'\n",
        "# os.makedirs(DRIVE_DIR, exist_ok=True)\n",
        "# import shutil\n",
        "# shutil.copy(model_path, os.path.join(DRIVE_DIR, 'model.keras'))\n",
        "# shutil.copy('scores.json', os.path.join(DRIVE_DIR, 'scores.json'))\n",
        "# shutil.copy('training_history.png', os.path.join(DRIVE_DIR, 'training_history.png'))\n",
        "# shutil.copy('confusion_matrix.png', os.path.join(DRIVE_DIR, 'confusion_matrix.png'))\n",
        "# print(f'Files saved to Google Drive: {DRIVE_DIR}')"
      ],
      "metadata": {
        "id": "save_drive"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Done!\n",
        "\n",
        "**On your local machine:**\n",
        "1. Place `model.keras` in `artifacts/training/`\n",
        "2. Place `scores.json` in project root\n",
        "3. Place plots in `artifacts/evaluation/`\n",
        "4. Commit and push"
      ],
      "metadata": {
        "id": "done"
      }
    }
  ]
}